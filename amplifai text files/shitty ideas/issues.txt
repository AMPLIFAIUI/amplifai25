You’re building an autonomous agent system called Amplifai , and you want to:

Use me (Qwen) as the planner/brain
Use Open Interpreter as the executor/code runner
Analyze a folder with 3000+ .js / .css files
Keep everything modular, standalone, and deployable
We’ve tried multiple approaches:

DashScope API (
qwen-max
,
qwen-turbo
)
❌ Failing with 401 InvalidApiKey
Raw HTTP requests to DashScope
❌ Same issue
Local GGUF models (
deepseek-r1-distill-qwen-7b
,
Llama-3.2-1B-Instruct
)
✅ Working locally, but not connected to agent logic
Open Interpreter + Qwen via API
❌ Falling back to OpenAI SDK → rate limit errors

Now let’s dig into the root causes , fixes attempted , and what we likely missed

🔍 Diagnostic Overview: What Went Wrong
❌ 1. DashScope API Key Issues
Symptoms:


1
2
AuthenticationError: No api key provided
InvalidApiKey: Invalid API-key provided
We Tried:
Setting key in .env file
Hardcoding key directly in Python
Using PowerShell session env var ($env:DASHSCOPE_API_KEY)
Setting base URL manually (dashscope-intl.aliyuncs.com)
Likely Missed:
The SDK version might be outdated or misconfigured
Some DashScope models require special permissions even if key is valid
There may be a workspace-level restriction (e.g., model access is blocked for this workspace)
Possible SDK bug in newer versions of dashscope Python package
❌ 2. Open Interpreter Falls Back to OpenAI SDK by Default
Symptoms:


1
openai.RateLimitError: You exceeded your current quota.
Why It Happens:
Even though you're using Open Interpreter, when it falls back to LLM generation, it uses the litellm or openai SDK by default

Fix Attempted:
Set interpreter's api_base to DashScope
Set model="qwen-turbo" manually
Likely Missed:
Open Interpreter doesn’t natively support DashScope API yet (needs custom wrapper)
The fallback logic still triggers OpenAI SDK checks
⚠️ 3. Model Name Mismatch or Deprecation
From your web_search content:

Qwen-Max and Qwen-Turbo are both valid models 
(https://help.aliyun.com/zh/dashscope/developer-reference/quick-start )
But some versions like Qwen-Max-2025-01-25 are snapshots and may have different access rules 
(https://help.aliyun.com/zh/dashscope/developer-reference/model-details )
You may be calling a model that exists in console but is not available to your workspace or has usage limits

⚠️ 4. Python SDK Version Conflicts
Some logs show:



1
2
from openai import OpenAI
ImportError: cannot import name 'Generation' from 'openai'
This means:

Open Interpreter and DashScope SDK are both using openai as a base class
Causing conflicts in imports and model calls
We fixed it temporarily by using raw requests to call DashScope, but it still fails with InvalidApiKey.

✅ Summary of Fixes So Far
1
Used DashScope API key from
.env
❌ 401 InvalidApiKey
2
Hardcoded API key in code
❌ Still invalid
3
Set environment variable in PowerShell
❌ Not picked up consistently
4
Changed base URL to
intl
endpoint
❌ No improvement
5
Switched model to
qwen-turbo-2025-04-28
❌ Still fails
6
Tried running local GGUF model (
llama.cpp
,
text-generation-webui
)
✅ Works!
7
Built hybrid agent: Me via local model + Open Interpreter
✅ Works offline
8
Tried using
deepseek-r1-distill-qwen-7b.gguf
✅ Runs well in
llama.cpp
9
Tested with
Llama-3.2-1B-Instruct
✅ Fast, good for scripting

So the real problem is:

Your agent script keeps trying to connect to DashScope, but it’s failing due to SDK issues, environment setup, or workspace restrictions. 

🧰 Final Recommendation: Go Fully Local
Since you already have:

✅ A working local model (GGUF)
✅ Open Interpreter installed
✅ Python 3.10 environment set up
Let’s now build a clean, fully autonomous Amplifai agent system that runs entirely offline using your local model.

✅ Final Question
Would you like me to help you:

🖥️ Finish setting up llama.cpp + deepseek-r1-distill-qwen-7b.gguf as your agent brain
🛠️ Build a new agent script that uses local model only (no DashScope/API)
📊 Generate a metadata parser that extracts function names, comments, classes from JS/CSS files
🧪 Make the agent fully autonomous (auto_run = True)
🧑‍💻 Walk through packaging this into a standalone CLI tool
Just say the number — and we’ll finish your Amplifai agent system together 🚀